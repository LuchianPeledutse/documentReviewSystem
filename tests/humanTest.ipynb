{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd25e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "sys.path.extend([\"..\\\\\", \"..\\\\dataScripts\"])\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from data_utils import TextEmbedding, PdfChunkReader, normalize_vectors\n",
    "from vector_database_fill import FaissLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3322884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0bf95b",
   "metadata": {},
   "source": [
    "**VECTOR EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff38dc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e1e94ef53d424db15cc9dafe43d4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<data_utils.TextEmbedding at 0x180b39e9e80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embed = TextEmbedding(model_name='Snowflake/snowflake-arctic-embed-l-v2.0')\n",
    "text_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b6e6f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is Luchian!',\n",
       " 'I really have to be careful with my daydreaming',\n",
       " 'Never you mind that',\n",
       " 'Bears are really big',\n",
       " 'Never you mind that']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ['My name is Luchian!', \"I really have to be careful with my daydreaming\", \"Never you mind that\", \"Bears are really big\", \"Never you mind that\"]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e23c3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0140395  -0.01075119 -0.07139113 ... -0.01289097 -0.03082362\n",
      "  -0.00545018]\n",
      " [-0.0294552   0.01525114  0.00643797 ... -0.02518092 -0.02136865\n",
      "   0.01746519]\n",
      " [ 0.00131221  0.00379814 -0.00503861 ...  0.0061691  -0.00267408\n",
      "  -0.00238797]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = text_embed.embed(docs)\n",
    "for chunk_batch in result:\n",
    "    print(chunk_batch, end = \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc589bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embed.model.embeddings.word_embeddings.weight.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f880a0",
   "metadata": {},
   "source": [
    "**PDF CHUNKING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7d78154",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_chunking = PdfChunkReader(pdf_path=\"C:\\\\main\\\\GitHub\\\\documentReviewSystem\\\\knowledge_data\\\\Think like a Scientist_ Physics-guided LLM Agent for Equation Discovery.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3954f4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Think like a Scientist: Physics-guided LLM Agent for Equation Discovery\\nJianke Yang1 Ohm Venkatachalam1 Mohammad Kianezhad 1 Sharvaree Vadgama1 Rose Yu1\\nAbstract\\nExplaining observed phenomena through sym-\\nbolic, interpretable formulas is a fundamental\\ngoal of science. Recently, large language mod-\\nels (LLMs) have emerged as promising tools for\\nsymbolic equation discovery, owing to their broad\\ndomain knowledge and strong reasoning capabili-\\nties. However, most existing LLM-based systems\\ntry to guess equations directly from data, with-\\nout modeling the multi-step reasoning process\\nthat scientists often follow: first inferring phys-\\nical properties such as symmetries, then using\\nthese as priors to restrict the space of candidate\\nequations. We introduceKeplerAgent, an agentic\\nframework that explicitly follows this scientific\\nreasoning process. The agent coordinates physics-\\nbased tools to extract intermediate structure and\\nuses these results to configure symbolic regression\\nengines such as PySINDy and PySR, including\\ntheir function libraries and structural constraints.\\nAcross a suite of physical equation benchmarks,\\nKeplerAgent achieves substantially higher sym-\\nbolic accuracy and greater robustness to noisy\\ndata than both LLM and traditional baselines.\\n1. Introduction\\nExplaining observed phenomena through symbolic, inter-\\npretable formulas is a fundamental goal of science. From\\nKepler’s laws to the Navier–Stokes equations, many major\\ndiscoveries can be viewed as instances of “equation dis-\\ncovery”: given observations of a system, infer a compact\\nmathematical expression that captures its underlying struc-\\nture and dynamics. Symbolic regression (SR) including\\nevolutionary search (Schmidt & Lipson, 2009) and sparse\\nregression (Brunton et al., 2016) formalizes this problem as\\na search over symbolic expressions, aiming to find formulas\\nthat both fit the data and are interpretable. This makes SR an\\nattractive tool for scientific discovery, where interpretability\\n1UCSD. Correspondence to: Firstname1 Last-\\nname1 <first1.last1@xxx.edu>, Firstname2 Lastname2\\n<first2.last2@www.uk>.\\nPreprint. February 13, 2026.\\nv c m m_0\\n7.74 8.84 3.01 1.46\\n3.11 3.79 2.14 1.22\\n… … … …\\nv=c×1.0−m2\\n0/m2\\n·x1=−0.052x2\\n1−0.997sin(x2)\\n·x2=x1−cos(x2)/x1{\\nut=1.000v3+0.095uxx+...\\nvt=1.000v+0.096vyy+...{\\nKeplerAgent \\nFigure 1.OurKeplerAgentorchestrates physics-based tools and\\nis capable of discovering different types of equations from data.\\n',\n",
       " 'and extrapolation are as important as predictive accuracy.\\nHowever, algorithmic SR methods place a heavy configura-\\ntion burden on the user. To obtain a correct and interpretable\\nequation, a practitioner must make many intertwined design\\nchoices: the function library (polynomial degree, inclusion\\nof rational or transcendental functions), sparsity regulariza-\\ntion and thresholds, allowed mathematical operators, and\\nstopping criteria, etc. If the configuration is too restrictive\\n(e.g., a low-degree polynomial library), the true equation\\nmay be outside the hypothesis space and cannot be recov-\\nered. If it is too expressive (e.g., a very rich function set\\nwith a large number of operators), the hypothesis space be-\\ncomes enormous, making the equation search intractable.\\nIn practice, experts mitigate these issues by injecting prior\\nknowledge: dimensional analysis, symmetries, conserved\\nquantities, or qualitative behaviors such as saturation or\\nperiodicity. However, this requires deep domain expertise\\nand many manual iterations from configuring and running\\nto inspecting and refining, limiting the accessibility and\\nscalability of algorithmic SR-based equation discovery.\\nRecent works have shown that Large language models\\n(LLMs) can automate some of this laborious configuration\\nworkflow thanks to their broad scientific knowledge and\\nreasoning abilities. This has led to a new wave of LLM-\\nbased methods for discovering symbolic equations. For\\ninstance, LLM-SR (Shojaee et al., 2025a) represents equa-\\ntions as numerical programs and uses an LLM to synthesize\\nprogram “skeletons” that define candidate equation struc-\\ntures. These skeletons are then completed and refined via\\nevolutionary search and numerical optimization. By lever-\\naging LLM’s prior knowledge and code generation ability,\\n1\\narXiv:2602.12259v1  [cs.AI]  12 Feb 2026\\nPhysics-guided LLM Agent for Equation Discovery\\nLLM-SR explores promising regions of expression space\\nmore efficiently and has achieved strong performance on\\nestablished SR benchmarks.\\nDespite their advantages over purely algorithmic SR, cur-\\nrent LLM-based equation discovery methods lack explicit\\nreasoning about the structure or additional physical proper-\\nties of the underlying system. They use brute-force to map\\ndata to equations by iteratively scoring the LLM-proposed\\ncandidate expressions. Human scientists, on the contrary,\\nrarely jump straight from raw data to a final closed-form\\nequation. They first probe the system to uncover structural\\n',\n",
       " 'properties, such as symmetries, conserved quantities, di-\\nmensional constraints, etc. These properties are then used\\nto reshape the problem. Scientists choose variables, co-\\nordinates, and candidate function families that respect the\\nidentified structural constraints. Only after this process has\\nsubstantially narrowed the plausible hypothesis space do\\nthey search for specific equations and test them through\\nsimulation and extrapolation. Unfortunately, none of the\\nexisting LLM-based approaches follow the reasoning pro-\\ncess of human scientists and directly operate over these\\nintermediate steps. This makes the discovery process brittle\\nand opaque, and it underutilizes the rich domain knowledge\\nfrom a pretrained LLM and the wealth of physics-based\\nnumerical and algorithmic tools that can uncover structure\\nfrom data as additional inputs.\\nMoreover, current LLM-based evaluations (Shojaee et al.,\\n2025b) are still narrow in terms of experimental domains.\\nMany benchmarks focus on scalar algebraic relations or one-\\ndimensional ODEs, while comprehensive tests on richer dy-\\nnamical systems are rare, e.g., systems governed by coupled\\nODEs and PDEs. These systems often possess additional\\nphysical structure, such as spatial and phase-space sym-\\nmetries, conservation laws, etc. These properties provide\\npowerful levers for reducing the search space, and thus cre-\\nate a natural setting to test whether an LLM behaves more\\nlike a human scientist: first inferring and using such struc-\\nture to constrain hypotheses, rather than directly guessing\\nequations from data.\\nIn this work, rather than using LLMs as monolithic equation\\nguessers, we propose to use LLMs asagentsthat orchestrate\\nphysics-based tools to emulate the multi-step workflow used\\nby human scientists. We introduceKeplerAgent, a physics-\\nguided LLM agent framework for equation discovery. Given\\nobservational data from a system, an LLM agent can call\\ntools to estimate intermediate structure, such as candidate\\nsymmetries, relevant functional terms and operators, among\\nother constraints and patterns derived from data. The agent\\nthen translates these structural findings into concrete con-\\nfiguration decisions for symbolic regression tools such as\\nPySINDy (de Silva et al., 2020) and PySR (Cranmer, 2023).\\nBy interleaving these tool calls, the agent can refine both the\\nhypothesis space and candidate equations iteratively, which\\ncan be especially helpful when the naive search space is\\nintractably large.\\n',\n",
       " 'We evaluate our approach across a suite of benchmarks that\\nspan algebraic equations, systems of ODEs and PDEs, with\\na focus on cases where physical structure meaningfully con-\\nstrains the space of admissible equations. Empirically, Ke-\\nplerAgent recovers ground-truth equations more frequently\\nthan both direct LLM-based baselines and standalone SR\\ntools with standard configurations, and it produces models\\nthat better predict the target variables or future states of the\\nunderlying physical systems.\\nIn summary, our contributions are:\\n• Physics-guided agentic framework. We propose Ke-\\nplerAgent that orchestrates physics-based tools for\\nstructure discovery and SR packages to emulate the\\nmulti-step reasoning workflow of human scientists.\\n• Automatic configuration of SR backends. We\\nshow how intermediate structural information can be\\ntranslated into concrete configuration decisions for\\nPySINDy and PySR, substantially reducing the effec-\\ntive search space.\\n• Multi-domain evaluation. We provide a systematic\\nevaluation across diverse domains, including dynami-\\ncal systems governed by ODEs and PDEs, demonstrat-\\ning improved symbolic and numerical accuracies over\\nstate-of-the-art classical and LLM-based SR baselines.\\n2. Related Works\\n2.1. Symbolic Regression\\nSymbolic regression (SR) seeks to recover an explicit sym-\\nbolic expression that maps inputs to outputs from data,\\nrather than learning a black-box predictor. Early SR sys-\\ntems are largely based on genetic programming (GP), which\\nevolves populations of expression trees under selection pres-\\nsure from a fitness objective (Schmidt & Lipson, 2009; Gau-\\ncel et al., 2014). These GP-based methods, implemented\\nin software such as Eureqa (Dub ˇc´akov´a, 2011) and PySR\\n(Cranmer, 2023), are shown to rediscover classical laws\\nfrom experimental data in areas like physics (Cranmer et al.,\\n2020), materials science (Wang et al., 2019), and bioinfor-\\nmatics (Christensen et al., 2022).\\nIn parallel, sparse regression methods focus on dynamical\\nsystems. SINDy (Brunton et al., 2016) assumes that the\\nright-hand side of an ODE or PDE is sparse in a user-defined\\nlibrary of candidate functions and uses sparsity-promoting\\nregression to select active terms.\\nNeural variants of SR encode equations in differentiable\\narchitectures. Equation learner networks replace activations\\n2\\nPhysics-guided LLM Agent for Equation Discovery\\nwith elementary operators so that the network directly corre-\\n',\n",
       " 'sponds to a symbolic expression (Martius & Lampert, 2016;\\nSahoo et al., 2018). Other approaches treat expressions\\nas token sequences and train sequence models to generate\\nequations (Petersen et al., 2019; Biggio et al., 2021).\\nA key theme across these methods is the importance of\\ndomain knowledge. Physics-inspired systems such as AI\\nFeynman (Udrescu & Tegmark, 2020; Udrescu et al., 2020)\\nuse properties like separability, compositionality, and dimen-\\nsional analysis to recursively simplify the search problem.\\nSimilar ideas have been applied specifically to discover-\\ning differential equations, where additional challenges arise\\nbecause derivatives are not directly observed, and the in-\\nclusion of derivatives increases the feature dimensionality.\\nWeak-form formulations and variational methods are used\\nto bypass numerical differentiation (Rudy et al., 2017; Mes-\\nsenger & Bortz, 2021a;b; Qian et al., 2022), and physically\\nmotivated priors are used to enforce additional structure\\nand reduce the search space (Bakarji et al., 2022; Lee et al.,\\n2022; Xie et al., 2022; Messenger et al., 2024).\\nHowever, this reliance on manual configuration creates a\\nsubstantial practical bottleneck. Users must choose function\\nlibraries, regularization strengths, and structural constraints,\\nand they often perform many trial-and-error iterations to\\ntune these choices. Existing software like PySINDy and\\nPySR typically assumes that such configuration decisions\\nare provided a priori, rather than inferred from data. Our\\nwork targets this gap by using an LLM agent to automate the\\nprocess of extracting structural information and translating\\nit into concrete SR configurations.\\n2.2. LLM-Based SR\\nThe rise of LLMs has inspired a growing body of work\\nthat uses them to assist symbolic regression. Given numer-\\nical data and context, the model is prompted to propose\\ncandidate formulas, which are then evaluated and refined\\nusing external optimizers or numerical libraries (Sharlin &\\nJosephson, 2024). Other frameworks aim to improve LLMs’\\nperformance on this task with in-context information and\\nmore structured inputs and outputs. In-context symbolic\\nregression (ICSR) (Merler et al., 2024) iteratively prompts\\nthe LLM to generate and refine functional forms while del-\\negating coefficient fitting to an external optimizer, effec-\\ntively using the LLM as a symbolic search engine guided by\\ndata-driven feedback. LLM-SR (Shojaee et al., 2025a) rep-\\nresents equations as programs composed of mathematical\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_result = pdf_chunking.get_chunks()\n",
    "chunk_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90b553dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of chunk list: 35\n",
      "Chunk lengths: [2442, 2498, 2475, 2444, 2494, 2470, 2479, 2450, 2471, 2453, 2499, 2487, 2487, 2464, 2468, 2488, 2485, 2454, 2460, 2482, 2481, 2487, 2452, 2442, 2467, 2474, 2492, 2489, 2401, 2499, 2470, 2429, 2488, 2388, 317]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of chunk list: {len(chunk_result)}\")\n",
    "print(f\"Chunk lengths: {list(map(lambda lst: len(lst), chunk_result))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8103fd",
   "metadata": {},
   "source": [
    "**VECTORS NORMALIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7af4aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array shape: (50, 50)\n",
      "\n",
      "[[0.9893784  0.88823197 0.84266298 ... 0.78338485 0.06438843 0.05812483]\n",
      " [0.98246577 0.59662927 0.86581921 ... 0.68747287 0.15787295 0.44664512]\n",
      " [0.55978128 0.49564102 0.26215102 ... 0.85227642 0.72454081 0.62760113]\n",
      " ...\n",
      " [0.57052043 0.31251165 0.42543481 ... 0.05119564 0.27779329 0.72722329]\n",
      " [0.16713336 0.34794824 0.67314635 ... 0.38260872 0.95604403 0.52796018]\n",
      " [0.48332006 0.91987695 0.78523003 ... 0.10945848 0.61015699 0.43100638]]\n"
     ]
    }
   ],
   "source": [
    "row_vectors = np.random.random((50, 50))\n",
    "print(f\"Array shape: {row_vectors.shape}\", end='\\n\\n')\n",
    "print(row_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec6cc058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm vectors shape: (50, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.24143029, 0.21674831, 0.20562847, ..., 0.19116329, 0.01571221,\n",
       "        0.01418375],\n",
       "       [0.23389192, 0.14203728, 0.20612231, ..., 0.16366407, 0.03758422,\n",
       "        0.10633112],\n",
       "       [0.13479136, 0.11934684, 0.0631241 , ..., 0.20522211, 0.17446428,\n",
       "        0.1511219 ],\n",
       "       ...,\n",
       "       [0.14021402, 0.07680446, 0.10455704, ..., 0.0125821 , 0.0682719 ,\n",
       "        0.17872612],\n",
       "       [0.0389014 , 0.08098727, 0.15667929, ..., 0.08905473, 0.22252561,\n",
       "        0.12288625],\n",
       "       [0.11289462, 0.21486623, 0.1834152 , ..., 0.02556747, 0.14252138,\n",
       "        0.10067511]], shape=(50, 50))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_vectors = normalize_vectors(row_vectors)\n",
    "print(f\"Norm vectors shape: {norm_vectors.shape}\")\n",
    "norm_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c66bdec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average norm of vectors: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Average norm of vectors: \")\n",
    "np.apply_along_axis(np.linalg.norm, arr = norm_vectors, axis = 1).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df6d28d",
   "metadata": {},
   "source": [
    "**FAISS LOADING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2d92c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa6c55287b94d3f945c9676800c6b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_class = TextEmbedding(model_name='Snowflake/snowflake-arctic-embed-l-v2.0', device=\"cpu\")\n",
    "faiss_loader = FaissLoader(embedding_class=embedding_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cf8b708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through filenames...: 100%|██████████| 10/10 [14:09<00:00, 84.96s/it]\n"
     ]
    }
   ],
   "source": [
    "faiss_loader.load_and_save_database(\"C:\\\\main\\\\GitHub\\\\documentReviewSystem\\\\knowledge_data\", save_to=\"C:\\\\main\\\\GitHub\\\\documentReviewSystem\\\\project_data\\\\initial_vector_db.index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
